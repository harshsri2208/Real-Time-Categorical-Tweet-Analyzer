{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this program is a test implementation of twitter sentiment analysis\n",
    "\n",
    "# importing important libraries\n",
    "\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "import tweepy \n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob\n",
    "from nltk.stem.porter import * \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4399\n",
      "2639\n",
      "1760\n"
     ]
    }
   ],
   "source": [
    "# creating Twitter Client class\n",
    "\n",
    "class TwitterClient(object): \n",
    "    ''' \n",
    "    Generic Twitter Class for sentiment analysis. \n",
    "    '''\n",
    "    def __init__(self): \n",
    "        ''' \n",
    "        Class constructor or initialization method. \n",
    "        '''\n",
    "        # keys and tokens from the Twitter Dev Console \n",
    "        consumer_key = 'CvYmsrHRg4T5px0PnS6ioRzXL'\n",
    "        consumer_secret = 'juaFtBJdjiLd24RF0UJbmryQsNZPIUoijEzzV4gskcY3a0Wm83'\n",
    "        access_token = '1187987957921144833-4dujoiCynlPUitJGPJkNDMBz2HyRRb'\n",
    "        access_token_secret = 'mx1vML6FgACmRRonakfMlbfOIxp2wNyYNqPu0U4X6puTV'\n",
    "  \n",
    "        # attempt authentication \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "            # set access token and secret \n",
    "            self.auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            self.api = tweepy.API(self.auth) \n",
    "        except: \n",
    "            print(\"Error: Authentication Failed\") \n",
    "  \n",
    "    def clean_tweet(self, tweet): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, special characters \n",
    "        using simple regex statements. \n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "  \n",
    "    def get_tweet_sentiment(self, tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(self.clean_tweet(tweet)) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity < 0: \n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "  \n",
    "    def get_tweets(self, query, count = 1000): \n",
    "        ''' \n",
    "        Main function to fetch tweets and parse them. \n",
    "        '''\n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "  \n",
    "        try: \n",
    "            # call twitter api to fetch tweets\n",
    "            \n",
    "            maxId=0\n",
    "            for i in range(100):\n",
    "                fetched_tweets = self.api.search(q = query, count = count,max_id=maxId) \n",
    "                maxId=fetched_tweets.max_id\n",
    "\n",
    "                # parsing tweets one by one \n",
    "                for tweet in fetched_tweets: \n",
    "                    # empty dictionary to store required params of a tweet \n",
    "                    parsed_tweet = {} \n",
    "\n",
    "                    # saving text of tweet \n",
    "                    parsed_tweet['text'] = tweet.text \n",
    "                    # saving sentiment of tweet \n",
    "                    parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) \n",
    "\n",
    "                    # appending parsed tweet to tweets list \n",
    "                    if tweet.retweet_count > 0: \n",
    "                        # if tweet has retweets, ensure that it is appended only once \n",
    "                        if parsed_tweet not in tweets: \n",
    "                            tweets.append(parsed_tweet) \n",
    "                    else: \n",
    "                        tweets.append(parsed_tweet) \n",
    "\n",
    "            # return parsed tweets \n",
    "            return tweets \n",
    "  \n",
    "        except tweepy.TweepError as e: \n",
    "            # print error (if any) \n",
    "            print(\"Error : \" + str(e))\n",
    "\n",
    "     \n",
    "    \n",
    "    \n",
    "api=TwitterClient()\n",
    "tweets=api.get_tweets(query=\"trump\")\n",
    "total_data=pd.read_csv('empty.csv')\n",
    "train=pd.read_csv('empty1.csv')\n",
    "test=pd.read_csv('empty2.csv')\n",
    "trainsize=int(len(tweets)*0.6)\n",
    "for i in range(len(tweets)):\n",
    "    if i<trainsize:\n",
    "        train.loc[i]=[i+1,tweets[i][\"sentiment\"],tweets[i][\"text\"]]\n",
    "    else:\n",
    "        test.loc[i]=[i+1,tweets[i][\"sentiment\"],tweets[i][\"text\"]]\n",
    "    total_data.loc[i]=[i+1,tweets[i][\"sentiment\"],tweets[i][\"text\"]]\n",
    "    i=i+1\n",
    "    \n",
    "total_data.head()\n",
    "\n",
    "# loading training and test data\n",
    "\n",
    "#train  = pd.read_csv('Dataset/train_tweets.csv')\n",
    "#test = pd.read_csv('Dataset/test_tweets.csv')\n",
    "\n",
    "#combining training and and test data for preprocessing \n",
    "#total_data = train.append(test, ignore_index=True)\n",
    "print(len(total_data))\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @pdmcleod: The tl;dr of the 300-page is thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @Catmandu50: Sedition: Pelosi Travels to Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>@AZachParkinson Trump kept the real racist out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @ddale8: No, Trump didn’t predict Brexit th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @TheWayWithAnoa: Dear White Moderates,\\n\\nH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id label                                              tweet\n",
       "0  1     0  RT @pdmcleod: The tl;dr of the 300-page is thi...\n",
       "1  2     0  RT @Catmandu50: Sedition: Pelosi Travels to Ma...\n",
       "2  3     0     @AZachParkinson Trump kept the real racist out\n",
       "3  4     1  RT @ddale8: No, Trump didn’t predict Brexit th...\n",
       "4  5     0  RT @TheWayWithAnoa: Dear White Moderates,\\n\\nH..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove @word pattern from the tweets as they do not add any value\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Removing  Twitter Handles \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @pdmcleod: The tl;dr of the 300-page is thi...</td>\n",
       "      <td>RT : The tl;dr of the 300-page is this line: “...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @Catmandu50: Sedition: Pelosi Travels to Ma...</td>\n",
       "      <td>RT : Sedition: Pelosi Travels to Madrid for UN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>@AZachParkinson Trump kept the real racist out</td>\n",
       "      <td>Trump kept the real racist out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @ddale8: No, Trump didn’t predict Brexit th...</td>\n",
       "      <td>RT : No, Trump didn’t predict Brexit the day b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @TheWayWithAnoa: Dear White Moderates,\\n\\nH...</td>\n",
       "      <td>RT : Dear White Moderates,\\n\\nHow many times d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id label                                              tweet  \\\n",
       "0  1     0  RT @pdmcleod: The tl;dr of the 300-page is thi...   \n",
       "1  2     0  RT @Catmandu50: Sedition: Pelosi Travels to Ma...   \n",
       "2  3     0     @AZachParkinson Trump kept the real racist out   \n",
       "3  4     1  RT @ddale8: No, Trump didn’t predict Brexit th...   \n",
       "4  5     0  RT @TheWayWithAnoa: Dear White Moderates,\\n\\nH...   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  RT : The tl;dr of the 300-page is this line: “...  \n",
       "1  RT : Sedition: Pelosi Travels to Madrid for UN...  \n",
       "2                     Trump kept the real racist out  \n",
       "3  RT : No, Trump didn’t predict Brexit the day b...  \n",
       "4  RT : Dear White Moderates,\\n\\nHow many times d...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing @username patterns from tweets using remove_pattern function\n",
    "\n",
    "print('\\n\\nRemoving  Twitter Handles \\n\\n')\n",
    "total_data['tidy_tweet'] = np.vectorize(remove_pattern)(total_data['tweet'], \"@[\\w]*\")\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Removing Short Words\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @pdmcleod: The tl;dr of the 300-page is thi...</td>\n",
       "      <td>tl;dr 300-page this line: “President Trump com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @Catmandu50: Sedition: Pelosi Travels to Ma...</td>\n",
       "      <td>Sedition: Pelosi Travels Madrid Climate Meetin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>@AZachParkinson Trump kept the real racist out</td>\n",
       "      <td>Trump kept real racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @ddale8: No, Trump didn’t predict Brexit th...</td>\n",
       "      <td>Trump didn’t predict Brexit before. didn’t per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @TheWayWithAnoa: Dear White Moderates,\\n\\nH...</td>\n",
       "      <td>Dear White Moderates, many times do... Black, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id label                                              tweet  \\\n",
       "0  1     0  RT @pdmcleod: The tl;dr of the 300-page is thi...   \n",
       "1  2     0  RT @Catmandu50: Sedition: Pelosi Travels to Ma...   \n",
       "2  3     0     @AZachParkinson Trump kept the real racist out   \n",
       "3  4     1  RT @ddale8: No, Trump didn’t predict Brexit th...   \n",
       "4  5     0  RT @TheWayWithAnoa: Dear White Moderates,\\n\\nH...   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  tl;dr 300-page this line: “President Trump com...  \n",
       "1  Sedition: Pelosi Travels Madrid Climate Meetin...  \n",
       "2                             Trump kept real racist  \n",
       "3  Trump didn’t predict Brexit before. didn’t per...  \n",
       "4  Dear White Moderates, many times do... Black, ...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words with small lenght i.e., words having length smaller than 3 hardly hold any sentiment\n",
    "# hence it is better to remove such words\n",
    "\n",
    "print('\\n\\nRemoving Short Words\\n\\n')\n",
    "total_data['tidy_tweet'] = total_data['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tweet Tokenization\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [tl;dr, 300-page, this, line:, “President, Tru...\n",
       "1    [Sedition:, Pelosi, Travels, Madrid, Climate, ...\n",
       "2                          [Trump, kept, real, racist]\n",
       "3    [Trump, didn’t, predict, Brexit, before., didn...\n",
       "4    [Dear, White, Moderates,, many, times, do..., ...\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separating each word as a token\n",
    "\n",
    "print('\\n\\nTweet Tokenization\\n\\n')\n",
    "tokenized_tweet = total_data['tidy_tweet'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stemming\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [tl;dr, 300-page, thi, line:, “presid, trump, ...\n",
       "1    [sedition:, pelosi, travel, madrid, climat, me...\n",
       "2                          [trump, kept, real, racist]\n",
       "3    [trump, didn’t, predict, brexit, before., didn...\n",
       "4    [dear, white, moderates,, mani, time, do..., b...\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming words i.e, words are play,playing,played are treated similarly\n",
    "\n",
    "print('\\n\\nStemming\\n\\n')\n",
    "stemmer = PorterStemmer()\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @pdmcleod: The tl;dr of the 300-page is thi...</td>\n",
       "      <td>tl;dr 300-page thi line: “presid trump comprom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @Catmandu50: Sedition: Pelosi Travels to Ma...</td>\n",
       "      <td>sedition: pelosi travel madrid climat meeting,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>@AZachParkinson Trump kept the real racist out</td>\n",
       "      <td>trump kept real racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @ddale8: No, Trump didn’t predict Brexit th...</td>\n",
       "      <td>trump didn’t predict brexit before. didn’t per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @TheWayWithAnoa: Dear White Moderates,\\n\\nH...</td>\n",
       "      <td>dear white moderates, mani time do... black, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id label                                              tweet  \\\n",
       "0  1     0  RT @pdmcleod: The tl;dr of the 300-page is thi...   \n",
       "1  2     0  RT @Catmandu50: Sedition: Pelosi Travels to Ma...   \n",
       "2  3     0     @AZachParkinson Trump kept the real racist out   \n",
       "3  4     1  RT @ddale8: No, Trump didn’t predict Brexit th...   \n",
       "4  5     0  RT @TheWayWithAnoa: Dear White Moderates,\\n\\nH...   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  tl;dr 300-page thi line: “presid trump comprom...  \n",
       "1  sedition: pelosi travel madrid climat meeting,...  \n",
       "2                             trump kept real racist  \n",
       "3  trump didn’t predict brexit before. didn’t per...  \n",
       "4  dear white moderates, mani time do... black, l...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stiching these tokens together\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "total_data['tidy_tweet'] = tokenized_tweet\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "label\n",
      "tweet\n",
      "tidy_tweet\n"
     ]
    }
   ],
   "source": [
    "# checking out the features of the total_data dataframe\n",
    "\n",
    "for col in total_data.columns:\n",
    "\tprint(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tidy_tweets column to numerical value using bag of words algorithm\n",
    "\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90,min_df=2,max_features=1000, stop_words='english')\n",
    "\n",
    "# bag-of-words feature matrix\n",
    "\n",
    "bow = bow_vectorizer.fit_transform(total_data['tidy_tweet'])\n",
    "\n",
    "bow=bow.toarray()\n",
    "print(bow.shape)\n",
    "\n",
    "# get an idea of bow array as it is difficult to visualise in normal form\n",
    "#hence converting it to a numpy array\n",
    "'''\n",
    "# implementing bag of words\n",
    "\n",
    "word2count = {}\n",
    "wordlist=[]\n",
    "for data in total_data['tidy_tweet'].values: \n",
    "    words = nltk.word_tokenize(data)\n",
    "    wordlist.append(words)\n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq \n",
    "freq_words = heapq.nlargest(1000, word2count, key=word2count.get)\n",
    "\n",
    "bow = [] \n",
    "for data,lword in zip(total_data['tidy_tweet'].values,wordlist): \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in lword: \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    bow.append(vector) \n",
    "bow = np.asarray(bow)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4399, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classes for logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and validation set\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(bow, total_data['label'],  test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3519, 1000)\n",
      "(880, 1000)\n",
      "(3519,)\n",
      "(880,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_bow.shape)\n",
    "print(xvalid_bow.shape)\n",
    "print(ytrain.shape)\n",
    "print(yvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg=LogisticRegression()\n",
    "lreg.fit(xtrain_bow,list(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(880,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict=lreg.predict(xvalid_bow)\n",
    "predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0\n",
      " 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1\n",
      " 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1\n",
      " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0\n",
      " 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
      " 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
      " 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "predict=lreg.predict(xvalid_bow)\n",
    "print(np.asarray(yvalid))\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5776699029126214\n"
     ]
    }
   ],
   "source": [
    "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "print(f1_score(list(yvalid), prediction_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
